{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, pipeline, AutoModelForImageClassification, AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "import accelerate\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(\"dtype:\", torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_audio_features(y, sr, output_path=\"spectrogram.png\"):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(mel_spec_db, x_axis='time', y_axis='mel', sr=sr)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Mel-frequency spectrogram')\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_huggingface_model():\n",
    "    model_name = \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "    model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    return feature_extractor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_emotion_huggingface(audio_path, feature_extractor, model):\n",
    "    waveform, sample_rate = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "\n",
    "    inputs = feature_extractor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        predicted_class_id = logits.argmax(-1).item()\n",
    "    \n",
    "    emotion = model.config.id2label[predicted_class_id]\n",
    "\n",
    "    scores = torch.nn.functional.softmax(logits, dim=1)[0].tolist()\n",
    "    emotion_scores = {model.config.id2label[i]: scores[i] for i in range(len(scores))}\n",
    "    \n",
    "    return emotion, emotion_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_audio_emotion(audio_path, use_huggingface=True, use_resnet=False, \n",
    "                         resnet_model_path=None, save_spectrogram=True):\n",
    "    try:\n",
    "        results = {\"audio_path\": audio_path}\n",
    "        \n",
    "        # HuggingFace model approach\n",
    "        if use_huggingface:\n",
    "            feature_extractor, hf_model = setup_huggingface_model()\n",
    "            emotion, scores = recognize_emotion_huggingface(audio_path, feature_extractor, hf_model)\n",
    "            results[\"huggingface_emotion\"] = emotion\n",
    "            results[\"huggingface_scores\"] = scores\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in analyze_audio_emotion: {e}\")\n",
    "        # Return an empty dictionary instead of None\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file: Crema_data\\1091_WSI_HAP_XX.wav\n",
      "Detected emotion (HuggingFace): happy\n",
      "Emotion confidence scores:\n",
      "  angry: 0.1296\n",
      "  calm: 0.1228\n",
      "  disgust: 0.1248\n",
      "  fearful: 0.1094\n",
      "  happy: 0.1360\n",
      "  neutral: 0.1283\n",
      "  sad: 0.1304\n",
      "  surprised: 0.1188\n"
     ]
    }
   ],
   "source": [
    "audio_file = \"Crema_data\\\\1091_WSI_HAP_XX.wav\"\n",
    "\n",
    "results = analyze_audio_emotion(\n",
    "    audio_file, \n",
    "    use_huggingface=True,\n",
    "    use_resnet=False,\n",
    "    save_spectrogram=True\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Audio file: {audio_file}\")\n",
    "if 'huggingface_emotion' in results:\n",
    "    print(f\"Detected emotion (HuggingFace): {results['huggingface_emotion']}\")\n",
    "    print(\"Emotion confidence scores:\")\n",
    "    for emotion, score in results['huggingface_scores'].items():\n",
    "        print(f\"  {emotion}: {score:.4f}\")\n",
    "\n",
    "if 'spectrogram_path' in results:\n",
    "    print(f\"Spectrogram saved to: {results['spectrogram_path']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
